{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# @author: tongzi\n",
    "# @description: combining different models for ensemble learing\n",
    "# @created date: 2019/09/04\n",
    "# @last modification: 2019/09/04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the movie dataset into more convenient format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyprind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = 'aclImdb'\n",
    "pbar = pyprind.ProgBar(50000)\n",
    "df = pd.DataFrame()\n",
    "labels = {'pos': 1, 'neg': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:08:09\n"
     ]
    }
   ],
   "source": [
    "for s in ('test', 'train'):\n",
    "    for label in ('pos', 'neg'):\n",
    "        path = os.path.join(basepath, s, label)\n",
    "        for file in os.listdir(path):\n",
    "            with open(os.path.join(path, file), 'r', encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt, labels[label]]], ignore_index=True)\n",
    "            pbar.update()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['review', 'sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df.to_csv('movie_data.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming words into feature vectors  \n",
    "To construct a bag-of-words model based on the word counts in the respective\n",
    "documents, we can use the CountVectorizer class implemented in scikit-learn. As\n",
    "we will see in the following code section, CountVectorizer takes an array of text\n",
    "data, which can be documents or sentences, and constructs the bag-of-words model\n",
    "for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = np.array([\n",
    "    'I am a boy',\n",
    "    'I like baseketball',\n",
    "    \"But I don't play baseketball for a long time\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag = count.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'am': 0,\n",
       " 'boy': 2,\n",
       " 'like': 6,\n",
       " 'baseketball': 1,\n",
       " 'but': 3,\n",
       " 'don': 4,\n",
       " 'play': 8,\n",
       " 'for': 5,\n",
       " 'long': 7,\n",
       " 'time': 9}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 1, 1, 1, 0, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values in the feature vectors are also called the raw term frequencies: $tf(t, d)$ —the number of times a term t occurs in a document d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">To summarize the concept of the n-gram representation, the 1-gram and 2-gram representations of our first document \"the sun is shining\" would be constructed as follows:  \n",
    "• 1-gram: \"the\", \"sun\", \"is\", \"shining\"  \n",
    "• 2-gram: \"the sun\", \"sun is\", \"is shining\"  \n",
    "The CountVectorizer class in scikit-learn allows us to use different n-gram models via its ngram_range parameter. While a 1-gram representation is used by default, we could switch to a 2-gram representation by initializing a new CountVectorizer instance with *ngram_range=(2,2)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing word relevancy via term frequency-inverse document frequency  \n",
    "  \n",
    "  When we are analyzing text data, we often encounter words that occur across\n",
    "multiple documents from both classes. These frequently occurring words typically\n",
    "don't contain useful or discriminatory information. In this subsection, we will learn about a useful technique called **term frequency-inverse document frequency (tf-idf)** that can be used to downweight these frequently occurring words in the feature vectors. The tf-idf can be defined as the product of the term frequency and the inverse document frequency:  \n",
    "$$tf - idf(t, d) = tf(t, d) \\times idf(t, d)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the $tf(t, d)$ is the term frequency that we introduced in the previous  section, and $idf(t, d)$ is the inverse document frequency and can be calculated as follows:  \n",
    "$$idf(t, d) = \\log \\frac{n_d}{1+df(d, t)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here $n_d$ is the total number of documents, and $df(d, t)$ is the number of documents $d$ that contain the term $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scikit-learn library implements yet another transformer, the TfidfTransformer\n",
    "class, that takes the raw term frequencies from the CountVectorizer class as input\n",
    "and transforms them into tf-idfs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.71 0.   0.71 0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.61 0.   0.   0.   0.   0.8  0.   0.   0.  ]\n",
      " [0.   0.3  0.   0.39 0.39 0.39 0.   0.39 0.39 0.39]]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.fit_transform(count.fit_transform(docs)).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we'd manually calculated the tf-idfs of the individual terms in our\n",
    "feature vectors, we'd notice that TfidfTransformer calculates the tf-idfs slightly\n",
    "differently compared to the standard textbook equations that we defined previously.\n",
    "The equations for the inverse document frequency implemented in scikit-learn is\n",
    "computed as follows:  \n",
    "$$idf(t, d) = \\log \\frac{1+n_d}{1+df(d, t)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the $tf-idf$ computed in scikit-learn deviates slightly from the default equation we defined earlier:  \n",
    "$$tf-idf(t,d) = tf(t, d) \\times \\left(idf(t,d)+1 \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default ( norm='l2' ), scikit-learn's TfidfTransformer applies the L2-normalization, which returns a vector of length 1 by dividing an un-normalized feature vector v by its L2-norm:  \n",
    "$$v_{norm} = \\frac{v}{\\lVert v \\rVert_2} = \\frac{v}{\\sqrt{v_1^2 + v_2^2 + ... + v_n^2}} = \\frac{v}{\\sqrt{\\sum_{i=1}^{n} v_i^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I went and saw this movie last night after being coaxed to by a few friends of mine. I'll admit that I was reluctant to see it because from what I knew of Ashton Kutcher he was only able to do comedy. I was wrong. Kutcher played the character of Jake Fischer very well, and Kevin Costner played Ben Randall with such professionalism. The sign of a good movie is that it can toy with our emotions. This one did exactly that. The entire theater (which was sold out) was overcome by laughter during the first half of the movie, and were moved to tears during the second half. While exiting the theater I not only saw many women in tears, but many full grown men as well, trying desperately not to let anyone see them crying. This movie was great, and I suggest that you go see it before you judge.\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, 'review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'and I suggest that you go see it before you judge.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, 'review'][-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', ''))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test :) :( :)'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(\"</a>This :) is :( a test :-)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " since we will make use of the cleaned text data over and over again during the\n",
    "next sections, let us now apply our preprocessor function to all the movie reviews\n",
    "in our DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
